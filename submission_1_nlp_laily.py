# -*- coding: utf-8 -*-
"""Submission_1_NLP_Laily.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C1aOhRjxq39rXJpkby5X53QPioRJVTi7

***Nama : Laily Khoirunnisa'***

*Klasifikasi Teks Data 'Emotion Detection'*

`Submission 1 NLP Dicoding kelas Pengembangan Machine Learning`

<h1> <strong> 1. Mengambil dataset dari Kaggle</strong></h1>
"""

# Colab library to upload files to notebook
from google.colab import files

# Install Kaggle library
!pip install -q kaggle
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d praveengovi/emotions-dataset-for-nlp

"""<h1> <strong> 2. Ekstrasi File Dataset </strong></h1>"""

!mkdir -p /tmp/emotions-dataset

# melakukan ekstraksi pada file zip
import zipfile,os
local_zip = 'emotions-dataset-for-nlp.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/emotions-dataset')
zip_ref.close()

"""<h1> <strong> 3. Preprosessing </strong></h1>"""

import pandas as pd
df = pd.read_csv('/tmp/emotions-dataset/train.txt', names=['sentence', 'label'], sep=';')
df.head()

df.tail()

num_data = df['sentence'].size
max_len_sentence = max(df['sentence'].str.len())
num_words_sentence = sum(df['sentence'].str.len())
num_unique_words_sentence = df['sentence'].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0).size

print('Jumlah data : ',num_data)
print('Panjang maksimum kalimat : ',max_len_sentence)
print('Jumlah kata dalam semua data di kolom kalimat : ',num_words_sentence)
print('Jumlah kata unik dalam semua data di kolom kalimat : ',num_unique_words_sentence)

category = pd.get_dummies(df.label)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='label')
df_baru

kalimat = df_baru['sentence'].values
label = df_baru[['anger', 'fear', 'joy', 'love', 'sadness','surprise']].values
print(kalimat)
print(label)

from sklearn.model_selection import train_test_split
kalimat_latih, kalimat_test, label_latih, label_test = train_test_split(kalimat, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=num_unique_words_sentence, oov_token='x')
tokenizer.fit_on_texts(kalimat_latih) 
tokenizer.fit_on_texts(kalimat_test)
 
sekuens_latih = tokenizer.texts_to_sequences(kalimat_latih)
sekuens_test = tokenizer.texts_to_sequences(kalimat_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

"""<h1><strong>4. Membuat Model</strong></h2>"""

import tensorflow as tf
from tensorflow.keras.layers import Dropout

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=num_unique_words_sentence, output_dim=64),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

"""<h1><strong>5. Deklarasi Callbacks</strong></h2>"""

#EarlyStopping and ModelCheckpoint

from keras.callbacks import EarlyStopping, ModelCheckpoint

es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 15)
mc = ModelCheckpoint('./model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

"""<h1><strong>6. Proses Training dan Pengujian</strong></h2>"""

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), 
                    verbose=1,
                    callbacks=[es, mc]
                    )

"""<h1><strong>7. Membuat Plot Tingkat Akurasi Data Training dan Validasi</strong></h2>"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(20)

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 8))
plt.subplot(2, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""<h1><strong>8. Prediksi Emosi Di Luar Dataset</strong></h2>"""

import re
import nltk 
nltk.download('punkt')
from nltk.tokenize import word_tokenize

def text_preprocess(text, stop_words=False):
  '''
  Accepts text (a single string) and
  a parameters of preprocessing
  Returns preprocessed text

  '''
  # clean text from non-words
  text = re.sub(r'\W+', ' ', text).lower()

  # tokenize the text
  tokens = word_tokenize(text)

  if stop_words:
    # delete stop_words
    tokens = [token for token in tokens if token not in STOPWORDS]

  return tokens

import numpy as np

def predict(texts):
  '''
  Accepts array if texts (strings)
  Prints sentence and the corresponding label (emotion)
  Returns nothing
  
  '''
  emotions = ['anger', 'fear', 'joy', 'love', 'sadness','surprise']

  texts_prepr = [text_preprocess(t) for t in texts]
  sequences = tokenizer.texts_to_sequences(texts_prepr)
  pad = pad_sequences(sequences)

  predictions = model.predict(pad)
  labels = np.argmax(predictions, axis=1)
  
  for i, lbl in enumerate(labels):
    print(texts[i],' --> ', emotions[lbl])

test_texts = ['I am so happy', 
              'The man felt lonely', 
              'The guests felt satisfied', 
              'He was really disappointed',
              'She surprised',
              'Not a day goes on without thinking of him',
              'She angry with him',
              'He feels intense fear when he has to fly']

predict(test_texts)